{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBP3PQhKSEQR"
   },
   "source": [
    "In this notebook I create a Recurrent Neural Network model based on the Long Short-Term Memory unit to train and benchmark on the Penn Treebank dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3ZsKOweSEQS"
   },
   "source": [
    "</b>Language Modelling</b> -- a very relevant task that is the cornerstone of many different linguistic problems such as <b>Speech Recognition, Machine Translation and Image Captioning</b>. For this, I will be using the Penn Treebank dataset, which is an often-used dataset for benchmarking Language Modelling models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnhbZ9kKSEQT"
   },
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8QBaKO8SEQW"
   },
   "source": [
    "I need <b><code>numpy</code></b> and <b><code>tensorflow</code></b>. Additionally, we can import directly the <b><code>tensorflow\\.models.rnn</code></b> model, which includes the function for building RNNs, and <b><code>tensorflow\\.models.rnn.ptb.reader</code></b> which is the helper module for getting the input data from the dataset we just downloaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOYcWuzJSEQW",
    "outputId": "36ba078e-d97e-4644-b0cd-1f4bb7b98f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2.0rc0\n",
      "  Downloading tensorflow-2.2.0rc0-cp37-cp37m-manylinux2010_x86_64.whl (515.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 515.9 MB 22 kB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (0.37.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (1.1.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (1.6.3)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 46.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (3.17.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (1.13.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (1.15.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (1.43.0)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 36.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (1.19.5)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (1.4.1)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "\u001b[K     |████████████████████████████████| 448 kB 76.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc0) (1.1.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (57.4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (4.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.10.0.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.1.1)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.4.0\n",
      "    Uninstalling gast-0.4.0:\n",
      "      Successfully uninstalled gast-0.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.7.0\n",
      "    Uninstalling tensorflow-2.7.0:\n",
      "      Successfully uninstalled tensorflow-2.7.0\n",
      "Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.1.1 tensorflow-2.2.0rc0 tensorflow-estimator-2.1.0\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2.0rc0\n",
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inaX1XlbSEQX"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "if not tf.__version__ == '2.2.0-rc0':\n",
    "    print(tf.__version__)\n",
    "    raise ValueError('please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdG8evr9SEQY"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir data/ptb\n",
    "!wget -q -O data/ptb/reader.py https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/labs/Week3/data/ptb/reader.py\n",
    "!cp data/ptb/reader.py . \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6pLa9zFSEQY"
   },
   "outputs": [],
   "source": [
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHT4STvuSEQZ"
   },
   "source": [
    "<h2>Building the LSTM model for Language Modeling</h2>\n",
    "\n",
    "I start building the model using TensorFlow. The very first thing is download and extract the <code>simple-examples</code> dataset, which can be done by executing the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mhL2MAhSEQZ",
    "outputId": "82a0f5fa-d13f-44f1-ca2b-543cdb27fca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-16 14:31:07--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
      "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: ‘simple-examples.tgz’\n",
      "\n",
      "simple-examples.tgz 100%[===================>]  33.25M  8.43MB/s    in 4.3s    \n",
      "\n",
      "2022-01-16 14:31:12 (7.70 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "!tar xzf simple-examples.tgz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9X4uXxjgSEQZ"
   },
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 30\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size= 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"data/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLBiNvWiSEQa"
   },
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9x-xyzzMSEQa",
    "outputId": "285065f6-1918-42ba-e7d8-83898ff583da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhZdIwuxSEQa",
    "outputId": "67934667-8f6d-458a-cc40-3a54af4bd368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "\n",
    "print(id_to_word(train_data[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjdrUBIcSEQb"
   },
   "source": [
    "Lets just read one mini-batch now and feed our network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZI8wfNsSEQb"
   },
   "outputs": [],
   "source": [
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "_input_data = first_touple[0]\n",
    "_targets = first_touple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLM5v4AfSEQb",
    "outputId": "a98b13f3-2f72-4c58-e15f-68c5a6d6cc06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cats77tqSEQb",
    "outputId": "c99452c9-d7b0-49f9-b295-ec37af7d69df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioJAkCw7SEQb"
   },
   "source": [
    "Lets look at 3 sentences of our input x:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEBX3nZFSEQc",
    "outputId": "b41f3aed-71c3-48b9-eced-a3df1d1c81d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605],\n",
       "       [   0, 1071,    4,    0,  185,   24,  368,   20,   31, 3109,  954,\n",
       "          12,    3,   21,    2, 2915,    2,   12,    3,   21]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VAQwAvASEQc",
    "outputId": "dce5bc19-b552-4eb4-880c-1d471502f6b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim']\n"
     ]
    }
   ],
   "source": [
    "print(id_to_word(_input_data[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSII_fr0SEQc"
   },
   "source": [
    "<b>embedding_lookup()</b> finds the embedded values for our batch of 30x20 words. It  goes to each row of <code>input_data</code>, and for each word in the row/sentence, finds the correspond vector in <code>embedding_dic<code>. <br>\n",
    "It creates a \\[30x20x200] tensor, so, the first element of <b>inputs</b> (the first sentence), is a matrix of 20x200, which each row of it, is vector representing a word in the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97N2m_E7SEQc"
   },
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, embeding_vector_size,batch_input_shape=(batch_size, num_steps),trainable=True,name=\"embedding_vocab\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULa-U1eUSEQc",
    "outputId": "249b3e8b-8fad-45fd-a893-f7751f7b97ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 20, 200), dtype=float32, numpy=\n",
       "array([[[ 0.04870662, -0.01193376,  0.00658649, ..., -0.04968027,\n",
       "         -0.03812311,  0.0402422 ],\n",
       "        [-0.03306731,  0.00457491,  0.03667506, ..., -0.00831813,\n",
       "         -0.01256456,  0.03503459],\n",
       "        [-0.00496637, -0.0070735 ,  0.03331048, ..., -0.00319834,\n",
       "         -0.00926016, -0.03706694],\n",
       "        ...,\n",
       "        [ 0.0372239 , -0.02372563,  0.00439869, ...,  0.04839167,\n",
       "          0.03670845,  0.02530028],\n",
       "        [ 0.00545583, -0.0073446 ,  0.0075757 , ...,  0.01225203,\n",
       "          0.01210945,  0.04445219],\n",
       "        [ 0.00871379, -0.0168138 , -0.03219406, ...,  0.02777426,\n",
       "          0.02916456,  0.01563765]],\n",
       "\n",
       "       [[-0.02266669, -0.04095539, -0.04280273, ..., -0.02549719,\n",
       "         -0.02029875, -0.02476766],\n",
       "        [-0.03118582,  0.00155712, -0.04534843, ...,  0.01529891,\n",
       "         -0.03374769,  0.02878617],\n",
       "        [ 0.01350128,  0.00672488, -0.02636375, ..., -0.02916452,\n",
       "         -0.02539345, -0.03887729],\n",
       "        ...,\n",
       "        [ 0.04199025, -0.00669532, -0.0172619 , ..., -0.01001606,\n",
       "         -0.02239137, -0.0271673 ],\n",
       "        [ 0.04856483,  0.01060613,  0.00157924, ...,  0.01190307,\n",
       "         -0.04416481, -0.03621359],\n",
       "        [-0.03645843, -0.0121126 , -0.02978246, ...,  0.00220109,\n",
       "          0.04980442,  0.02655585]],\n",
       "\n",
       "       [[-0.02789819, -0.00637526, -0.02321539, ..., -0.03955258,\n",
       "         -0.03246324,  0.0231402 ],\n",
       "        [ 0.03283861, -0.01184481, -0.0497238 , ...,  0.02075973,\n",
       "         -0.03339546, -0.00208899],\n",
       "        [-0.01538992, -0.01075615,  0.04344385, ..., -0.04607992,\n",
       "          0.02011654, -0.00421048],\n",
       "        ...,\n",
       "        [-0.01926137,  0.04376414,  0.02796301, ..., -0.04859407,\n",
       "          0.03833077, -0.01649249],\n",
       "        [ 0.02152021, -0.03455272,  0.03047427, ..., -0.00679575,\n",
       "          0.02041366, -0.04898787],\n",
       "        [ 0.01886347,  0.02265782, -0.04342128, ...,  0.01128472,\n",
       "          0.02101586, -0.03203421]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.02251377,  0.01290102, -0.02693531, ...,  0.01120346,\n",
       "         -0.00395348,  0.02602759],\n",
       "        [-0.03462633,  0.0313716 ,  0.03722331, ...,  0.03780458,\n",
       "          0.00532099, -0.03147519],\n",
       "        [ 0.03574301, -0.02214916,  0.03502275, ..., -0.02472063,\n",
       "         -0.01127994, -0.02161427],\n",
       "        ...,\n",
       "        [ 0.04149092,  0.01828212,  0.02970545, ..., -0.02457461,\n",
       "          0.01803244, -0.01523496],\n",
       "        [-0.01907618,  0.03842534, -0.02952798, ..., -0.03163298,\n",
       "         -0.02406122,  0.03017699],\n",
       "        [ 0.00515133,  0.012988  ,  0.00689285, ..., -0.01259725,\n",
       "         -0.01655021, -0.0306499 ]],\n",
       "\n",
       "       [[-0.04719073, -0.00434224,  0.01326369, ...,  0.01645945,\n",
       "         -0.01102253,  0.032756  ],\n",
       "        [-0.03533899, -0.00969112,  0.02564767, ..., -0.04672081,\n",
       "          0.00826886, -0.00767116],\n",
       "        [-0.01538992, -0.01075615,  0.04344385, ..., -0.04607992,\n",
       "          0.02011654, -0.00421048],\n",
       "        ...,\n",
       "        [ 0.01775101, -0.02121214,  0.01929322, ...,  0.03308532,\n",
       "         -0.00809105,  0.02448976],\n",
       "        [-0.03118582,  0.00155712, -0.04534843, ...,  0.01529891,\n",
       "         -0.03374769,  0.02878617],\n",
       "        [ 0.01467854, -0.00059425,  0.03580179, ...,  0.0498575 ,\n",
       "          0.02402233, -0.03661997]],\n",
       "\n",
       "       [[-0.00804215,  0.02899395, -0.00069295, ..., -0.02112391,\n",
       "          0.04079178,  0.00106326],\n",
       "        [ 0.02915   , -0.0160169 ,  0.03733511, ..., -0.00639533,\n",
       "          0.04324751, -0.02095686],\n",
       "        [-0.02014179, -0.04278944, -0.02464349, ...,  0.00213625,\n",
       "         -0.04296942, -0.04063938],\n",
       "        ...,\n",
       "        [-0.01082715,  0.02245674,  0.01429144, ...,  0.02089782,\n",
       "          0.0251854 , -0.0087303 ],\n",
       "        [-0.01577155,  0.01433625, -0.04360697, ..., -0.02907407,\n",
       "         -0.00203854, -0.03912418],\n",
       "        [ 0.03451758, -0.02059841,  0.04880532, ...,  0.03123457,\n",
       "          0.01276818, -0.03325423]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define where to get the data for our embeddings from\n",
    "inputs = embedding_layer(_input_data)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71J2Od__SEQd"
   },
   "source": [
    "<h3>Constructing Recurrent Neural Networks</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IKXkERkSEQd"
   },
   "source": [
    "In this step, I create the stacked LSTM using <b>tf.keras.layers.StackedRNNCells</b>, which is a 2 layer LSTM network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wkV1N6QSEQd"
   },
   "outputs": [],
   "source": [
    "lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
    "lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZOPQP0DSEQd"
   },
   "outputs": [],
   "source": [
    "stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz7ocIEWSEQd"
   },
   "source": [
    "<b>tf.keras.layers.RNN</b> creates a recurrent neural network using <b>stacked_lstm</b>.\n",
    "\n",
    "The input should be a Tensor of shape: \\[batch_size, max_time, embedding_vector_size], in our case it would be (30, 20, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJXYd9GRSEQd"
   },
   "outputs": [],
   "source": [
    "layer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSF9podvSEQe"
   },
   "source": [
    "Also, we initialize the states of the nework:\n",
    "\n",
    "<h4>_initial_state</h4>\n",
    "\n",
    "For each LSTM, there are 2 state matrices, c_state and m_state.  c_state and m_state represent \"Memory State\" and \"Cell State\". Each hidden layer, has a vector of size 30, which keeps the states. so, for 200 hidden units in each LSTM, we have a matrix of size \\[30x200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVJaJtSBSEQe"
   },
   "outputs": [],
   "source": [
    "init_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Bd4ehoYSEQe"
   },
   "outputs": [],
   "source": [
    "layer.inital_state = init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xv0KNlezSEQe",
    "outputId": "9b38a86d-c5b1-49e4-f4c9-55f74e55a126"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(30, 200) dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.inital_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScU-iSRVSEQf"
   },
   "source": [
    "The output of the stackedLSTM comes from 128 hidden_layer, and in each time step(=20), one of them get activated. we use the linear activation to map the 128 hidden layer to a \\[30X20 matrix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8cKWtXkSEQf"
   },
   "outputs": [],
   "source": [
    "outputs = layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8FDeDHmSEQf",
    "outputId": "789d786c-3688-4f9b-9e53-41061e17a06c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 20, 128), dtype=float32, numpy=\n",
       "array([[[-9.6469698e-04, -8.2536967e-04, -4.6314072e-04, ...,\n",
       "         -4.3411154e-04,  6.1145984e-04, -5.8975833e-04],\n",
       "        [-1.2868774e-03, -1.3076189e-03, -7.4472441e-04, ...,\n",
       "         -1.0691351e-03,  1.5305374e-04, -2.1255064e-04],\n",
       "        [-1.1594163e-03, -2.4171208e-03, -9.4795099e-04, ...,\n",
       "          4.2319567e-05, -1.7404248e-04,  1.2326690e-05],\n",
       "        ...,\n",
       "        [-1.7939245e-03, -5.9895050e-03, -3.9084491e-04, ...,\n",
       "          8.1310160e-03, -2.2896552e-03, -8.5368368e-04],\n",
       "        [-2.8794527e-03, -6.1599035e-03,  1.0960505e-03, ...,\n",
       "          8.1292819e-03, -8.2695618e-04, -1.8090757e-05],\n",
       "        [-2.6664443e-03, -5.6500221e-03,  3.3125945e-03, ...,\n",
       "          8.2190223e-03,  4.2107164e-05, -4.8991543e-04]],\n",
       "\n",
       "       [[ 1.6534826e-04, -1.3182656e-03, -4.1052344e-04, ...,\n",
       "         -7.0256654e-05, -1.1013820e-03,  7.9689402e-04],\n",
       "        [ 1.9702839e-04, -2.3519869e-03, -1.7266523e-03, ...,\n",
       "         -8.0936600e-04, -4.2261597e-04,  1.6580416e-03],\n",
       "        [ 1.0951513e-03, -3.4660669e-03, -1.6403202e-03, ...,\n",
       "         -2.3583954e-03, -4.2949518e-04,  2.6131717e-03],\n",
       "        ...,\n",
       "        [ 1.5023878e-03,  2.0451806e-03, -5.4404819e-03, ...,\n",
       "         -9.0791914e-04,  7.2816033e-03, -3.5422188e-04],\n",
       "        [ 2.2748450e-03,  2.1701138e-03, -5.4441015e-03, ...,\n",
       "         -1.9758483e-03,  9.1680605e-03, -2.7537509e-04],\n",
       "        [ 3.1736791e-03,  2.2041590e-03, -4.3768985e-03, ...,\n",
       "         -3.1368874e-03,  1.0189689e-02, -3.9653803e-04]],\n",
       "\n",
       "       [[-2.6514742e-04, -8.5745199e-04, -6.8873829e-05, ...,\n",
       "          1.1321128e-03, -4.3982317e-04, -7.7864231e-04],\n",
       "        [-3.8038348e-04, -1.5259277e-03, -1.3408052e-04, ...,\n",
       "          1.3320748e-03, -9.1553702e-05, -1.4184380e-03],\n",
       "        [-4.5466350e-04, -2.1325636e-03, -9.0682728e-04, ...,\n",
       "          6.0158491e-04, -1.6655607e-04, -1.8561932e-03],\n",
       "        ...,\n",
       "        [-3.2587844e-04, -4.4952039e-04, -5.0740661e-03, ...,\n",
       "          4.0641259e-03,  2.8514054e-03,  1.8612876e-04],\n",
       "        [-9.5856580e-04,  4.8378730e-04, -5.2411132e-03, ...,\n",
       "          3.6137013e-03,  3.4921623e-03,  7.5504062e-04],\n",
       "        [-1.3219108e-03,  1.8326383e-03, -4.9286517e-03, ...,\n",
       "          3.4679184e-03,  4.3435609e-03, -9.1344293e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.6712968e-04,  6.9552421e-04, -2.1087096e-04, ...,\n",
       "          4.4553995e-04, -2.7387252e-04,  3.7586584e-04],\n",
       "        [ 1.1820415e-03,  5.2234024e-04, -8.8868639e-04, ...,\n",
       "          7.6415442e-04, -1.0593169e-03,  7.9353066e-04],\n",
       "        [ 1.3186127e-03,  8.9124916e-04, -1.0244489e-03, ...,\n",
       "          5.6959881e-05, -1.1963230e-04,  1.7568581e-03],\n",
       "        ...,\n",
       "        [-2.3669857e-03, -1.7486768e-03, -4.2729687e-05, ...,\n",
       "          3.4744777e-03, -3.8225274e-03, -3.9101412e-04],\n",
       "        [-1.0953543e-03, -2.8922821e-03, -1.5709226e-03, ...,\n",
       "          4.3542706e-03, -4.5269141e-03,  3.7693267e-04],\n",
       "        [-1.8208630e-04, -2.1130410e-03, -1.9219453e-03, ...,\n",
       "          4.2679780e-03, -5.0354386e-03,  5.2475585e-05]],\n",
       "\n",
       "       [[ 9.0189150e-04,  7.1362557e-04,  3.2945003e-04, ...,\n",
       "         -6.3038635e-04, -1.0212734e-03, -2.1305184e-04],\n",
       "        [ 3.0852109e-04,  4.4026101e-04, -6.4088893e-04, ...,\n",
       "         -2.5916088e-04, -2.8832881e-03, -8.4249717e-05],\n",
       "        [ 5.4396778e-06,  2.5582098e-04, -1.8917168e-03, ...,\n",
       "         -4.7236480e-04, -3.8753080e-03, -2.6515528e-04],\n",
       "        ...,\n",
       "        [-1.0600772e-03, -3.3110250e-03,  8.6435431e-04, ...,\n",
       "          3.8236461e-03,  3.2148452e-04,  5.0014430e-03],\n",
       "        [-8.1953011e-04, -3.8408432e-03,  1.4906793e-04, ...,\n",
       "          2.7137974e-03,  1.2088080e-03,  5.3123394e-03],\n",
       "        [-2.1971290e-03, -3.1405068e-03, -8.0461201e-04, ...,\n",
       "          9.7815711e-05,  1.2893016e-03,  5.4736850e-03]],\n",
       "\n",
       "       [[-1.9482986e-04, -8.5773406e-04, -9.9184038e-04, ...,\n",
       "         -1.0638618e-03, -8.8030484e-04,  4.4910150e-04],\n",
       "        [-9.1742596e-04, -3.4949908e-04, -1.9405123e-03, ...,\n",
       "         -1.5540731e-03, -2.3094888e-03,  3.8237995e-04],\n",
       "        [-1.5556235e-03,  1.5451327e-03, -1.7210367e-03, ...,\n",
       "         -2.5478206e-03, -2.2689172e-03, -2.6304771e-05],\n",
       "        ...,\n",
       "        [-1.9813050e-03,  4.4981749e-03,  1.9641337e-03, ...,\n",
       "         -3.2530185e-03,  4.6169651e-03,  1.9081433e-03],\n",
       "        [-1.1909563e-03,  4.4586975e-03,  3.2621480e-03, ...,\n",
       "         -3.7177077e-03,  4.3012430e-03,  2.8308411e-03],\n",
       "        [-6.0755742e-04,  3.9766133e-03,  2.4305882e-03, ...,\n",
       "         -2.8482547e-03,  3.4247199e-03,  3.5910020e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_r1RrT-eSEQf"
   },
   "source": [
    "<h2>Dense layer</h2>\n",
    "\n",
    "Now create densely-connected neural network layer that would reshape the outputs tensor from  [30 x 20 x 128] to [30 x 20 x 10000].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kZPtYzdSEQf"
   },
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OA7Whg5YSEQf"
   },
   "outputs": [],
   "source": [
    "logits_outputs  = dense(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkzsyWyESEQf",
    "outputId": "abf2ca11-cf30-4cd2-de1b-8e60e0ba8b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output from dense layer:  (30, 20, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the output from dense layer: \", logits_outputs.shape) #(batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-AtL5cySEQf"
   },
   "source": [
    "<h2>Activation layer</h2>\n",
    "\n",
    "A softmax activation layers is also then applied to derive the probability of the output being in any of the multiclass(10000 in this case) possibilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WZK_x70SEQf"
   },
   "outputs": [],
   "source": [
    "activation = tf.keras.layers.Activation('softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-u4eY9-SEQg"
   },
   "outputs": [],
   "source": [
    "output_words_prob = activation(logits_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-6YhbuqSEQg",
    "outputId": "f441e3fe-68ed-4179-b82e-dc2fb4d27e11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output from the activation layer:  (30, 20, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the output from the activation layer: \", output_words_prob.shape) #(batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_49zwvuSEQg"
   },
   "source": [
    "Lets look at the probability of observing words for t=0 to t=20:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pL4vjzZHSEQg",
    "outputId": "c10afb39-db69-49fe-eddc-7fd9482a2f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of observing words in t=0 to t=20 tf.Tensor(\n",
      "[[1.00001831e-04 1.00005440e-04 1.00015423e-04 ... 1.00008110e-04\n",
      "  1.00021971e-04 1.00010133e-04]\n",
      " [1.00012483e-04 1.00029851e-04 1.00026606e-04 ... 1.00023928e-04\n",
      "  1.00014156e-04 1.00024445e-04]\n",
      " [1.00023426e-04 1.00038822e-04 1.00024554e-04 ... 1.00030513e-04\n",
      "  1.00016150e-04 9.99973490e-05]\n",
      " ...\n",
      " [1.00049219e-04 9.99556869e-05 9.99562690e-05 ... 9.99481854e-05\n",
      "  1.00094643e-04 9.99026670e-05]\n",
      " [1.00058285e-04 9.99316617e-05 9.99727054e-05 ... 9.99444965e-05\n",
      "  1.00088459e-04 9.99178956e-05]\n",
      " [1.00047902e-04 9.99193653e-05 9.99946933e-05 ... 9.99361509e-05\n",
      "  1.00080040e-04 9.99256590e-05]], shape=(20, 10000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0,0:num_steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0c9ii0pSEQg"
   },
   "source": [
    "<h3>Prediction</h3>\n",
    "What is the word correspond to the probability output? Lets use the maximum probability:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtTrY5Z-SEQg",
    "outputId": "caabf9cb-9c51-4465-af88-961641808371"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6765, 4494, 4494, 8060, 1065, 1065, 2144, 9904, 9904, 5606, 6568,\n",
       "        352, 6982, 6982, 6982, 6982, 6982, 4316, 4316, 4316])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_words_prob[0,0:num_steps], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UurT0T95SEQg"
   },
   "source": [
    "So, what is the ground truth for the first word of first sentence? You can get it from target tensor, if you want to find the embedding vector:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogEzBVWrSEQg",
    "outputId": "9d98fd69-3094-4159-9b4d-9f6d6990e9cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjHPwYs1SEQh"
   },
   "outputs": [],
   "source": [
    "def crossentropy(y_true, y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykvSfjOjSEQh"
   },
   "outputs": [],
   "source": [
    "loss  = crossentropy(_targets, output_words_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HKWELI_SEQh"
   },
   "source": [
    "Lets look at the first 10 values of loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IHcIamKgSEQh",
    "outputId": "7b73645d-49c4-41b9-9933-8f6ac0d411d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([9.21046 , 9.210031, 9.210526, 9.210196, 9.210417, 9.20994 ,\n",
       "       9.210451, 9.21033 , 9.209973, 9.210232], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[0,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1f_KgkMSEQh"
   },
   "source": [
    "Now, we define cost as average of the losses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AAVMos8vSEQh",
    "outputId": "1cbb17df-ceca-4b77-fb28-ba9e83838726"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=184.20805>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss / batch_size)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht48kRU2SEQh"
   },
   "source": [
    "<h3>Training</h3>\n",
    "\n",
    "To do training for our network, we have to take the following steps:\n",
    "\n",
    "<ol>\n",
    "    <li>Define the optimizer.</li>\n",
    "    <li>Assemble layers to build model.</li>\n",
    "    <li>Calculate the gradients based on the loss function.</li>\n",
    "    <li>Apply the optimizer to the variables/gradients tuple.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5LPu9dvSEQh"
   },
   "source": [
    "<h4>1. Define Optimizer</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElzyxU6TSEQi"
   },
   "outputs": [],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "optimizer = tf.keras.optimizers.SGD(lr=lr, clipnorm=max_grad_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqjEPkXLSEQi"
   },
   "source": [
    "<h4>2. Assemble layers to build model.</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqW2N5WISEQi",
    "outputId": "ddd39cc2-e8c9-4242-a663-eb90e3bbc2f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_vocab (Embedding)  (30, 20, 200)             2000000   \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (30, 20, 128)             671088    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (30, 20, 10000)           1290000   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (30, 20, 10000)           0         \n",
      "=================================================================\n",
      "Total params: 3,961,088\n",
      "Trainable params: 3,955,088\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(layer)\n",
    "model.add(dense)\n",
    "model.add(activation)\n",
    "model.compile(loss=crossentropy, optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQlVgkzKSEQi"
   },
   "source": [
    "<h4>2. Trainable Variables</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oi_UKXAOSEQi"
   },
   "source": [
    "Defining a variable, if you passed <i>trainable=True</i>, the variable constructor automatically adds new variables to the graph collection <b>GraphKeys.TRAINABLE_VARIABLES</b>. Now, using <i>tf.trainable_variables()</i> you can get all variables created with <b>trainable=True</b>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-31qf9PiSEQi"
   },
   "outputs": [],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "tvars = model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxFvXd7tSEQi"
   },
   "source": [
    "Note: we can find the name and scope of all variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZ4kqoSKSEQi",
    "outputId": "355f1b75-e693-4093-94db-4a3c87da990f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_vocab/embeddings:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/recurrent_kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/bias:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/recurrent_kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/bias:0',\n",
       " 'dense/kernel:0',\n",
       " 'dense/bias:0']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mGarSxGSEQi"
   },
   "source": [
    "<h4>3. Calculate the gradients based on the loss function</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l23HIICXSEQj"
   },
   "outputs": [],
   "source": [
    "x = tf.constant(1.0)\n",
    "y =  tf.constant(2.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    g.watch(y)\n",
    "    func_test = 2 * x * x + 3 * x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FccarWNSSEQj",
    "outputId": "7d04e175-e434-4615-9fe6-cbc02ca9e2c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "var_grad = g.gradient(func_test, x) # Will compute to 10.0\n",
    "print(var_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySetVg68SEQj",
    "outputId": "4fe58bfe-a5bf-45d0-98d1-50e9daab99ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "var_grad = g.gradient(func_test, y) # Will compute to 3.0\n",
    "print(var_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIt1AsfeSEQj"
   },
   "source": [
    "Now, we can look at gradients w\\.r.t all variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dkCS91-SEQk"
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass.\n",
    "    output_words_prob = model(_input_data)\n",
    "    # Loss value for this batch.\n",
    "    loss  = crossentropy(_targets, output_words_prob)\n",
    "    cost = tf.reduce_sum(loss,axis=0) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLMGHI4KSEQk"
   },
   "outputs": [],
   "source": [
    "# Get gradients of loss wrt the trainable variables.\n",
    "grad_t_list = tape.gradient(cost, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jeiCah7LSEQk",
    "outputId": "ebfd77f7-bed1-4245-8b2b-24a8baf934c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7f6c309e4ed0>, <tf.Tensor: shape=(200, 1024), dtype=float32, numpy=\n",
      "array([[ 4.2663459e-07, -6.1952989e-07, -9.2177302e-08, ...,\n",
      "        -2.2093963e-07,  3.9344943e-07, -1.8651519e-08],\n",
      "       [ 5.6239850e-07,  4.0840618e-08, -1.0229735e-07, ...,\n",
      "        -3.0300029e-07,  2.0135602e-07,  2.7928004e-07],\n",
      "       [ 7.6816309e-07,  8.3982201e-07, -6.2940444e-08, ...,\n",
      "         3.6399160e-07,  1.4516289e-08, -6.1554104e-09],\n",
      "       ...,\n",
      "       [-8.2077321e-07,  7.8317845e-07, -3.3058512e-07, ...,\n",
      "         6.0170936e-07,  1.1440124e-07,  1.4874279e-07],\n",
      "       [ 8.5152135e-07,  1.0094560e-06,  5.5223086e-07, ...,\n",
      "         7.2387923e-07,  2.6708335e-08,  6.9982612e-08],\n",
      "       [-2.4198414e-07,  7.6622075e-07,  1.8744362e-07, ...,\n",
      "        -7.3234764e-07,  2.9273892e-07,  3.4699632e-08]], dtype=float32)>, <tf.Tensor: shape=(256, 1024), dtype=float32, numpy=\n",
      "array([[-2.9210300e-08, -5.5433045e-08, -3.6456406e-08, ...,\n",
      "         3.4376455e-08, -7.8433558e-08, -5.1977459e-08],\n",
      "       [ 5.1272675e-08, -5.3908366e-08, -6.2678779e-08, ...,\n",
      "        -9.8086836e-09, -3.9766952e-08, -1.1849922e-07],\n",
      "       [-7.3631945e-08,  5.5214571e-08, -2.2571570e-08, ...,\n",
      "         1.3346356e-07, -3.9747661e-08,  4.4307875e-08],\n",
      "       ...,\n",
      "       [-3.2767709e-09,  1.1796970e-07,  4.5184017e-08, ...,\n",
      "         5.6250485e-07, -9.8993468e-08, -3.1485083e-08],\n",
      "       [ 4.2306688e-08, -1.3270230e-07,  1.8143014e-07, ...,\n",
      "         1.2870313e-08, -4.1268726e-07, -5.9436079e-08],\n",
      "       [-7.4036990e-08, -4.1093358e-08, -4.8115254e-09, ...,\n",
      "         2.7644711e-08, -1.2856800e-07, -5.8945560e-09]], dtype=float32)>, <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
      "array([ 1.22385745e-05,  2.01765906e-05,  6.80227367e-06, ...,\n",
      "       -2.90612115e-05, -3.03357774e-05, -2.09422087e-05], dtype=float32)>, <tf.Tensor: shape=(256, 512), dtype=float32, numpy=\n",
      "array([[ 1.7512593e-07,  1.1590909e-07,  1.8018872e-08, ...,\n",
      "        -5.4187524e-07, -7.6239814e-10,  3.2368064e-10],\n",
      "       [-1.2809599e-07, -1.3908122e-08, -2.2375208e-07, ...,\n",
      "        -2.3971367e-07, -2.0185760e-07,  3.1275619e-08],\n",
      "       [ 2.2580281e-07,  1.9049754e-08, -7.3799050e-08, ...,\n",
      "        -1.7421834e-07, -1.7970052e-08,  1.3078218e-07],\n",
      "       ...,\n",
      "       [-1.6026654e-08,  1.8097946e-07,  2.1249777e-07, ...,\n",
      "        -1.0232615e-07, -3.1487737e-09, -1.2369767e-07],\n",
      "       [-9.2474991e-08,  1.4656383e-07, -1.0601744e-07, ...,\n",
      "        -3.1325669e-07,  6.6226406e-08,  2.3402080e-07],\n",
      "       [-4.0153060e-07, -1.0607361e-07,  5.9457200e-08, ...,\n",
      "         2.9928600e-07, -6.2541702e-08,  4.1205105e-08]], dtype=float32)>, <tf.Tensor: shape=(128, 512), dtype=float32, numpy=\n",
      "array([[-2.08075676e-07,  2.10969340e-08, -3.35996901e-08, ...,\n",
      "         1.97391842e-07,  4.99447097e-08,  1.31121908e-07],\n",
      "       [ 1.68865768e-07, -1.39701172e-07,  8.52735340e-08, ...,\n",
      "        -2.79119874e-08, -2.36470488e-08, -1.04585894e-07],\n",
      "       [ 5.19203880e-08,  1.15674410e-07,  1.37884868e-07, ...,\n",
      "         1.85179658e-07,  6.83031729e-08, -1.37267875e-07],\n",
      "       ...,\n",
      "       [ 2.05999608e-08, -4.68939092e-08,  1.33552547e-07, ...,\n",
      "         6.09164488e-08,  4.92339893e-08, -1.02525078e-07],\n",
      "       [ 5.55570381e-08,  1.55123672e-07,  3.58210670e-08, ...,\n",
      "        -1.03529040e-07,  7.67173418e-08, -1.15698327e-07],\n",
      "       [-1.10583890e-07, -5.48225643e-08, -2.18309992e-08, ...,\n",
      "        -9.12180198e-08,  2.13379447e-08,  6.26943830e-08]], dtype=float32)>, <tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
      "array([-2.15056316e-05, -1.65283416e-06, -1.52319935e-05, -6.56604925e-06,\n",
      "        2.43191298e-05, -4.40764561e-05,  1.93228952e-05,  3.57983845e-06,\n",
      "       -3.09267489e-05,  1.89193543e-05,  1.00489951e-05, -1.58104049e-05,\n",
      "        3.66877703e-06,  6.32611336e-05, -1.48444633e-06, -1.57213090e-05,\n",
      "        1.64918547e-05, -3.04677960e-05,  6.33793752e-05,  1.43845273e-05,\n",
      "       -3.11372314e-05,  1.84308010e-05, -3.98462544e-06, -2.44149815e-05,\n",
      "       -4.09994682e-05,  1.13819897e-05, -2.90305616e-05,  1.82315307e-05,\n",
      "        2.77753843e-05,  4.71245476e-05,  3.65118321e-05,  1.76003396e-05,\n",
      "        4.49363361e-05, -2.69649499e-05, -1.99600508e-05, -2.76346400e-05,\n",
      "        9.27212659e-06,  7.30344764e-05, -1.94394270e-05,  1.23874110e-04,\n",
      "       -3.48483491e-06, -1.93788092e-05, -9.76812589e-06, -3.85989779e-06,\n",
      "        6.17385740e-05,  1.03928578e-05, -8.69071664e-06,  5.27234006e-05,\n",
      "        1.49463749e-05,  2.32116872e-05,  6.18545891e-06,  2.99275143e-05,\n",
      "       -3.62514693e-05, -4.88308524e-06,  3.76927419e-05,  3.88909029e-05,\n",
      "        2.29165125e-05,  3.30024377e-05,  3.16701517e-05, -1.30196931e-05,\n",
      "        2.98760115e-05,  2.08887359e-05, -7.02821708e-05, -5.67457482e-06,\n",
      "        1.49456318e-05, -4.16463772e-05, -3.30547082e-05,  5.65635382e-05,\n",
      "       -2.42671558e-05,  4.33909299e-05,  5.80986671e-06,  1.63564800e-05,\n",
      "       -7.05595357e-06,  2.91172182e-05, -9.47421449e-06,  2.98407986e-06,\n",
      "       -3.06395577e-06, -3.97640761e-05, -2.62119120e-05,  6.58481440e-05,\n",
      "       -1.79999879e-05, -2.48664437e-05,  1.72374348e-05,  1.98149373e-05,\n",
      "        4.11722885e-06,  1.82547410e-07, -8.79299478e-09,  1.56838723e-05,\n",
      "       -2.70485252e-05,  1.20353561e-05,  4.98150257e-07,  2.17530724e-05,\n",
      "       -3.28779133e-05,  4.48023966e-05,  1.95596840e-05, -2.65624149e-05,\n",
      "       -3.29603208e-05,  8.99093175e-06,  6.27270538e-06, -1.76802550e-05,\n",
      "       -2.01373723e-06, -6.77135358e-06,  2.93643025e-05, -3.68933906e-05,\n",
      "       -1.25843835e-05,  3.49874281e-05,  3.40991173e-05,  2.37991699e-05,\n",
      "        5.03555530e-05, -1.26235664e-05, -5.74268597e-07,  2.87488274e-05,\n",
      "        3.32706077e-06,  2.07453377e-05, -9.80670575e-06,  4.64992718e-06,\n",
      "        3.41191408e-05, -1.70787989e-05,  2.13834574e-05, -1.59399588e-05,\n",
      "        1.92949847e-06, -4.38323805e-05, -7.57900079e-06, -3.40058705e-07,\n",
      "        1.59414631e-05, -4.54728361e-05, -2.49687946e-05,  3.79407829e-05,\n",
      "       -2.29751822e-05, -1.38957421e-05, -9.07674803e-06,  2.55422310e-06,\n",
      "        2.68055483e-05, -7.37779046e-05,  1.72989367e-05,  4.00235058e-06,\n",
      "        7.79871698e-07,  3.73360454e-05,  1.70748208e-05,  6.12921576e-06,\n",
      "       -1.31490779e-05,  1.04830571e-04,  2.47733697e-05, -7.97682515e-06,\n",
      "        1.12036450e-05, -6.34470634e-05,  6.71798116e-05,  2.67494088e-05,\n",
      "       -4.35792645e-05,  3.21193766e-05, -1.94975037e-05,  1.76920730e-06,\n",
      "       -8.95208359e-06,  1.50965134e-05, -1.33005451e-05,  4.37368953e-06,\n",
      "        2.38185494e-05,  7.95917949e-05,  2.74639278e-05,  6.42078739e-05,\n",
      "        7.17504154e-05, -2.60208944e-05, -4.80363160e-05, -5.04618729e-05,\n",
      "        1.11597819e-05,  1.01941070e-04, -3.26735280e-05,  1.41599507e-04,\n",
      "       -5.29088493e-06, -6.53614261e-05, -2.02389674e-05, -1.45051190e-05,\n",
      "        8.06961907e-05,  2.21618993e-05,  2.52636892e-06,  8.02025752e-05,\n",
      "        2.16084172e-05,  3.21120315e-05,  5.84927875e-06,  4.55067129e-05,\n",
      "       -3.96586001e-05,  1.60855489e-05,  6.52410235e-05,  5.75824379e-05,\n",
      "        4.38452007e-05,  2.85061524e-05,  3.79753801e-05,  1.13455244e-05,\n",
      "        3.01510408e-05,  4.06334693e-05, -1.26347964e-04, -2.39573274e-05,\n",
      "        2.53092730e-05, -5.18338857e-05, -6.37709891e-05,  7.65780715e-05,\n",
      "       -1.25090564e-05,  8.14221203e-05, -1.24898961e-05,  3.60769009e-05,\n",
      "       -4.11715246e-06,  3.04820351e-05, -1.43245688e-05, -4.30154523e-06,\n",
      "        5.71307828e-06, -2.89595628e-05, -2.43832928e-05,  1.17929798e-04,\n",
      "       -6.08248683e-07, -1.12459038e-05,  4.13469752e-05,  2.52555365e-05,\n",
      "        1.70916974e-06,  2.16947446e-05,  3.22071173e-06,  1.70495568e-05,\n",
      "       -2.52753016e-05,  4.43016143e-06, -1.33368267e-05,  2.20624861e-06,\n",
      "       -5.60179833e-05,  4.50732296e-05,  4.60442388e-05, -2.20180409e-05,\n",
      "       -4.09037748e-05,  1.01963069e-05,  2.75044695e-05, -3.11256226e-05,\n",
      "       -6.43348631e-06,  3.26310255e-05,  3.49882357e-05, -5.20063695e-05,\n",
      "       -1.91484469e-06,  6.33736636e-05,  4.54645633e-05,  3.33781463e-05,\n",
      "        3.93177215e-05, -2.55373889e-05,  1.72276032e-05,  3.04722671e-05,\n",
      "       -1.49775988e-05,  3.99141391e-05, -3.32201162e-05,  1.58910589e-05,\n",
      "        3.44203327e-05, -3.59550104e-05,  3.09306852e-05, -5.33117272e-05,\n",
      "        5.75141075e-06, -4.56216367e-05, -2.33805222e-05,  2.49429504e-05,\n",
      "        1.66744048e-05, -5.82080611e-05, -2.61530349e-05,  5.46894298e-05,\n",
      "       -3.03255431e-02, -9.27280821e-03,  3.71255726e-03,  1.77572239e-02,\n",
      "        5.14910445e-02, -3.57702002e-02, -4.70069237e-04, -6.66274503e-03,\n",
      "       -8.27321783e-02,  3.34014781e-02,  2.59053279e-02, -1.37947649e-02,\n",
      "       -2.39729136e-02, -3.42800766e-02,  5.72739821e-03,  4.46246332e-03,\n",
      "        2.79650558e-03, -2.05830857e-02, -8.11454374e-05, -2.69050878e-02,\n",
      "       -5.50746471e-02,  5.21034822e-02,  9.85850580e-04, -9.43125784e-03,\n",
      "        1.31082674e-02,  1.35403010e-03,  2.55269222e-02,  4.29406054e-02,\n",
      "        6.23494983e-02,  5.11969700e-02, -2.38488335e-02,  3.65090445e-02,\n",
      "       -2.85569802e-02, -2.14523263e-02,  5.65223210e-03, -1.51455291e-02,\n",
      "        3.65008623e-03,  2.58169994e-02, -4.89205234e-02, -6.49816617e-02,\n",
      "       -9.87647194e-03,  3.38237509e-02, -1.67735200e-02, -2.65226886e-03,\n",
      "        1.76485032e-02,  5.01379371e-03, -1.44388638e-02,  3.87708023e-02,\n",
      "        4.87057269e-02, -2.98601221e-02,  1.07009793e-02, -1.52412616e-02,\n",
      "        4.82623372e-03, -1.52285649e-02, -4.17344347e-02,  2.16019526e-02,\n",
      "        2.36267187e-02,  1.77263133e-02,  2.29721293e-02,  6.43844204e-03,\n",
      "       -1.34658730e-02, -2.40064207e-02,  5.10220528e-02, -3.77224609e-02,\n",
      "        8.65941495e-03, -4.34512421e-02, -4.48486134e-02, -4.85840291e-02,\n",
      "       -1.17153926e-02, -5.46478033e-02, -2.33373838e-03, -3.42414454e-02,\n",
      "        8.95448402e-03,  1.05112605e-02,  1.85862929e-03, -3.18704620e-02,\n",
      "       -2.39404570e-03,  1.63655840e-02, -1.59465112e-02, -4.01078798e-02,\n",
      "        4.01170505e-03,  4.74918075e-03,  3.44600230e-02,  2.41945721e-02,\n",
      "        9.16864909e-03, -4.48442250e-03, -6.13810960e-04,  2.01666225e-02,\n",
      "       -7.70002045e-03,  1.24781672e-02,  4.42587622e-02,  2.84377113e-02,\n",
      "       -1.06466180e-02, -8.45859386e-03, -2.43186355e-02,  3.09462585e-02,\n",
      "       -1.54787097e-02,  2.26804093e-02, -3.26688215e-02,  5.60978148e-03,\n",
      "       -2.98607107e-02,  3.63994054e-02,  7.74438353e-03,  6.11351579e-02,\n",
      "       -5.06837443e-02,  3.97911519e-02, -2.87216268e-02, -1.49703790e-02,\n",
      "        2.95817293e-03, -6.10448867e-02,  7.61223165e-03,  3.44506465e-04,\n",
      "       -9.53020900e-03,  6.49862224e-03, -2.68518645e-03,  1.53880185e-02,\n",
      "        5.75920474e-03, -2.49681436e-02,  9.58592631e-04, -4.78258543e-03,\n",
      "       -4.18232791e-02, -1.26932543e-02,  1.51064862e-02, -7.26579223e-03,\n",
      "        2.92200968e-02,  2.78978460e-02,  2.39193235e-02,  1.85779277e-02,\n",
      "       -2.43629984e-05, -6.02703767e-06, -1.35558384e-05, -2.50664243e-06,\n",
      "        2.22862618e-05, -5.16405125e-05,  2.12392806e-05, -4.36396840e-06,\n",
      "       -3.45555454e-05,  2.50757003e-05,  1.14835893e-05, -1.31005954e-05,\n",
      "        1.34986403e-05,  6.78115321e-05,  1.20450604e-05, -1.35948194e-05,\n",
      "        2.18889490e-05, -4.41627017e-05,  6.93055554e-05,  1.40105531e-05,\n",
      "       -3.23589520e-05,  2.34813378e-05, -3.09780398e-06, -2.93753892e-05,\n",
      "       -3.65427040e-05,  1.45666663e-05, -3.06527982e-05,  1.65969068e-05,\n",
      "        2.02926331e-05,  4.88623809e-05,  3.84354789e-05,  2.01445346e-05,\n",
      "        4.97662768e-05, -4.05573737e-05, -1.71466381e-05, -1.95012781e-05,\n",
      "        9.70386100e-06,  7.79475013e-05, -2.99746625e-05,  1.33306428e-04,\n",
      "       -1.09388848e-05, -2.04463431e-05, -1.70588646e-05, -5.08181165e-06,\n",
      "        6.53427342e-05,  1.71009015e-06, -8.20731384e-06,  5.43849274e-05,\n",
      "        2.18942623e-05,  2.71695899e-05,  1.96632755e-06,  3.04822606e-05,\n",
      "       -3.71831084e-05, -2.24520454e-06,  4.20209435e-05,  4.58329923e-05,\n",
      "        2.42368078e-05,  4.79995069e-05,  3.75350937e-05, -3.10702671e-06,\n",
      "        3.56249657e-05,  1.67793278e-05, -7.97668836e-05, -8.14832219e-06,\n",
      "        6.29496753e-06, -4.57860515e-05, -4.03206977e-05,  5.65566443e-05,\n",
      "       -2.97182232e-05,  5.46251140e-05,  6.23133974e-06,  1.54133850e-05,\n",
      "       -7.13064583e-06,  3.11227705e-05, -1.97783884e-05,  4.71117801e-06,\n",
      "       -2.79761252e-06, -4.02858823e-05, -2.02769297e-05,  7.14677590e-05,\n",
      "       -2.55518898e-05, -2.46356558e-05,  2.51447309e-05,  2.21545306e-05,\n",
      "       -1.17082964e-05, -8.47913543e-06,  7.37673508e-06,  2.30570186e-05,\n",
      "       -3.48263457e-05,  1.39586955e-05, -5.30526449e-07,  2.10971521e-05,\n",
      "       -5.08858429e-05,  5.16670625e-05,  2.43848172e-05, -2.65299532e-05,\n",
      "       -3.92839029e-05,  5.39301709e-06,  1.94434633e-05, -2.39071887e-05,\n",
      "       -6.51875325e-06, -1.06330426e-05,  2.88253941e-05, -3.77258766e-05,\n",
      "       -1.99564765e-05,  4.63582837e-05,  4.05097016e-05,  3.62673854e-05,\n",
      "        5.30836041e-05, -1.81459782e-05,  5.17266926e-07,  3.17030535e-05,\n",
      "        4.33244986e-06,  2.40426998e-05, -2.13108942e-05, -4.55484223e-06,\n",
      "        3.33145108e-05, -1.63590557e-05,  2.70043383e-05, -1.78784194e-05,\n",
      "        8.19086381e-07, -4.33043133e-05, -9.21024912e-06,  1.48488834e-06,\n",
      "        1.14691629e-05, -4.69654879e-05, -2.16555400e-05,  4.18804557e-05],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(128, 10000), dtype=float32, numpy=\n",
      "array([[-2.5997562e-03, -3.1275689e-03, -1.5074344e-03, ...,\n",
      "         3.8862872e-06,  3.8878334e-06,  3.8874286e-06],\n",
      "       [ 1.2456398e-03,  1.8508717e-03,  2.0183986e-03, ...,\n",
      "        -3.0386054e-06, -3.0402402e-06, -3.0391072e-06],\n",
      "       [ 1.6745620e-03,  2.5880071e-03,  1.6192558e-03, ...,\n",
      "        -4.3551618e-06, -4.3559039e-06, -4.3573559e-06],\n",
      "       ...,\n",
      "       [ 3.2339629e-04,  4.5403844e-04,  4.4332299e-04, ...,\n",
      "        -4.2573348e-08, -3.8772455e-08, -4.3857213e-08],\n",
      "       [ 2.4436420e-05,  2.8553896e-04, -7.4503332e-04, ...,\n",
      "         2.9450746e-08,  2.9750765e-08,  2.8350428e-08],\n",
      "       [-3.2629148e-05,  6.9734547e-04, -9.6071913e-04, ...,\n",
      "         5.2688722e-07,  5.2590434e-07,  5.2768053e-07]], dtype=float32)>, <tf.Tensor: shape=(10000,), dtype=float32, numpy=\n",
      "array([-0.7979985 , -1.0313317 , -1.0313317 , ...,  0.00200004,\n",
      "        0.00200058,  0.00200059], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(grad_t_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suKXEwo9SEQk"
   },
   "source": [
    "now, we have a list of tensors, t-list. We can use it to find clipped tensors. <b>clip_by_global_norm</b> clips values of multiple tensors by the ratio of the sum of their norms.\n",
    "\n",
    "<b>clip_by_global_norm</b> get <i>t-list</i> as input and returns 2 things:\n",
    "\n",
    "<ul>\n",
    "    <li>a list of clipped tensors, so called <i>list_clipped</i></li> \n",
    "    <li>the global norm (global_norm) of all tensors in t_list</li> \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvWksYIOSEQk",
    "outputId": "bcc6b306-4519-4fa2-ce60-cd54d7585730"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x7f6c3099f150>,\n",
       " <tf.Tensor: shape=(200, 1024), dtype=float32, numpy=\n",
       " array([[ 4.2663459e-07, -6.1952989e-07, -9.2177302e-08, ...,\n",
       "         -2.2093963e-07,  3.9344943e-07, -1.8651519e-08],\n",
       "        [ 5.6239850e-07,  4.0840618e-08, -1.0229735e-07, ...,\n",
       "         -3.0300029e-07,  2.0135602e-07,  2.7928004e-07],\n",
       "        [ 7.6816309e-07,  8.3982201e-07, -6.2940444e-08, ...,\n",
       "          3.6399160e-07,  1.4516289e-08, -6.1554104e-09],\n",
       "        ...,\n",
       "        [-8.2077321e-07,  7.8317845e-07, -3.3058512e-07, ...,\n",
       "          6.0170936e-07,  1.1440124e-07,  1.4874279e-07],\n",
       "        [ 8.5152135e-07,  1.0094560e-06,  5.5223086e-07, ...,\n",
       "          7.2387923e-07,  2.6708335e-08,  6.9982612e-08],\n",
       "        [-2.4198414e-07,  7.6622075e-07,  1.8744362e-07, ...,\n",
       "         -7.3234764e-07,  2.9273892e-07,  3.4699632e-08]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(256, 1024), dtype=float32, numpy=\n",
       " array([[-2.9210300e-08, -5.5433045e-08, -3.6456406e-08, ...,\n",
       "          3.4376455e-08, -7.8433558e-08, -5.1977459e-08],\n",
       "        [ 5.1272675e-08, -5.3908366e-08, -6.2678779e-08, ...,\n",
       "         -9.8086836e-09, -3.9766952e-08, -1.1849922e-07],\n",
       "        [-7.3631945e-08,  5.5214571e-08, -2.2571570e-08, ...,\n",
       "          1.3346356e-07, -3.9747661e-08,  4.4307875e-08],\n",
       "        ...,\n",
       "        [-3.2767709e-09,  1.1796970e-07,  4.5184017e-08, ...,\n",
       "          5.6250485e-07, -9.8993468e-08, -3.1485083e-08],\n",
       "        [ 4.2306688e-08, -1.3270230e-07,  1.8143014e-07, ...,\n",
       "          1.2870313e-08, -4.1268726e-07, -5.9436079e-08],\n",
       "        [-7.4036990e-08, -4.1093358e-08, -4.8115254e-09, ...,\n",
       "          2.7644711e-08, -1.2856800e-07, -5.8945560e-09]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
       " array([ 1.22385745e-05,  2.01765906e-05,  6.80227367e-06, ...,\n",
       "        -2.90612115e-05, -3.03357774e-05, -2.09422087e-05], dtype=float32)>,\n",
       " <tf.Tensor: shape=(256, 512), dtype=float32, numpy=\n",
       " array([[ 1.7512593e-07,  1.1590909e-07,  1.8018872e-08, ...,\n",
       "         -5.4187524e-07, -7.6239814e-10,  3.2368064e-10],\n",
       "        [-1.2809599e-07, -1.3908122e-08, -2.2375208e-07, ...,\n",
       "         -2.3971367e-07, -2.0185760e-07,  3.1275619e-08],\n",
       "        [ 2.2580281e-07,  1.9049754e-08, -7.3799050e-08, ...,\n",
       "         -1.7421834e-07, -1.7970052e-08,  1.3078218e-07],\n",
       "        ...,\n",
       "        [-1.6026654e-08,  1.8097946e-07,  2.1249777e-07, ...,\n",
       "         -1.0232615e-07, -3.1487737e-09, -1.2369767e-07],\n",
       "        [-9.2474991e-08,  1.4656383e-07, -1.0601744e-07, ...,\n",
       "         -3.1325669e-07,  6.6226406e-08,  2.3402080e-07],\n",
       "        [-4.0153060e-07, -1.0607361e-07,  5.9457200e-08, ...,\n",
       "          2.9928600e-07, -6.2541702e-08,  4.1205105e-08]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(128, 512), dtype=float32, numpy=\n",
       " array([[-2.08075676e-07,  2.10969340e-08, -3.35996901e-08, ...,\n",
       "          1.97391842e-07,  4.99447097e-08,  1.31121908e-07],\n",
       "        [ 1.68865768e-07, -1.39701172e-07,  8.52735340e-08, ...,\n",
       "         -2.79119874e-08, -2.36470488e-08, -1.04585894e-07],\n",
       "        [ 5.19203880e-08,  1.15674410e-07,  1.37884868e-07, ...,\n",
       "          1.85179658e-07,  6.83031729e-08, -1.37267875e-07],\n",
       "        ...,\n",
       "        [ 2.05999608e-08, -4.68939092e-08,  1.33552547e-07, ...,\n",
       "          6.09164488e-08,  4.92339893e-08, -1.02525078e-07],\n",
       "        [ 5.55570381e-08,  1.55123672e-07,  3.58210670e-08, ...,\n",
       "         -1.03529040e-07,  7.67173418e-08, -1.15698327e-07],\n",
       "        [-1.10583890e-07, -5.48225643e-08, -2.18309992e-08, ...,\n",
       "         -9.12180198e-08,  2.13379447e-08,  6.26943830e-08]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
       " array([-2.15056316e-05, -1.65283416e-06, -1.52319935e-05, -6.56604925e-06,\n",
       "         2.43191298e-05, -4.40764561e-05,  1.93228952e-05,  3.57983845e-06,\n",
       "        -3.09267489e-05,  1.89193543e-05,  1.00489951e-05, -1.58104049e-05,\n",
       "         3.66877703e-06,  6.32611336e-05, -1.48444633e-06, -1.57213090e-05,\n",
       "         1.64918547e-05, -3.04677960e-05,  6.33793752e-05,  1.43845273e-05,\n",
       "        -3.11372314e-05,  1.84308010e-05, -3.98462544e-06, -2.44149815e-05,\n",
       "        -4.09994682e-05,  1.13819897e-05, -2.90305616e-05,  1.82315307e-05,\n",
       "         2.77753843e-05,  4.71245476e-05,  3.65118321e-05,  1.76003396e-05,\n",
       "         4.49363361e-05, -2.69649499e-05, -1.99600508e-05, -2.76346400e-05,\n",
       "         9.27212659e-06,  7.30344764e-05, -1.94394270e-05,  1.23874110e-04,\n",
       "        -3.48483491e-06, -1.93788092e-05, -9.76812589e-06, -3.85989779e-06,\n",
       "         6.17385740e-05,  1.03928578e-05, -8.69071664e-06,  5.27234006e-05,\n",
       "         1.49463749e-05,  2.32116872e-05,  6.18545891e-06,  2.99275143e-05,\n",
       "        -3.62514693e-05, -4.88308524e-06,  3.76927419e-05,  3.88909029e-05,\n",
       "         2.29165125e-05,  3.30024377e-05,  3.16701517e-05, -1.30196931e-05,\n",
       "         2.98760115e-05,  2.08887359e-05, -7.02821708e-05, -5.67457482e-06,\n",
       "         1.49456318e-05, -4.16463772e-05, -3.30547082e-05,  5.65635382e-05,\n",
       "        -2.42671558e-05,  4.33909299e-05,  5.80986671e-06,  1.63564800e-05,\n",
       "        -7.05595357e-06,  2.91172182e-05, -9.47421449e-06,  2.98407986e-06,\n",
       "        -3.06395577e-06, -3.97640761e-05, -2.62119120e-05,  6.58481440e-05,\n",
       "        -1.79999879e-05, -2.48664437e-05,  1.72374348e-05,  1.98149373e-05,\n",
       "         4.11722885e-06,  1.82547410e-07, -8.79299478e-09,  1.56838723e-05,\n",
       "        -2.70485252e-05,  1.20353561e-05,  4.98150257e-07,  2.17530724e-05,\n",
       "        -3.28779133e-05,  4.48023966e-05,  1.95596840e-05, -2.65624149e-05,\n",
       "        -3.29603208e-05,  8.99093175e-06,  6.27270538e-06, -1.76802550e-05,\n",
       "        -2.01373723e-06, -6.77135358e-06,  2.93643025e-05, -3.68933906e-05,\n",
       "        -1.25843835e-05,  3.49874281e-05,  3.40991173e-05,  2.37991699e-05,\n",
       "         5.03555530e-05, -1.26235664e-05, -5.74268597e-07,  2.87488274e-05,\n",
       "         3.32706077e-06,  2.07453377e-05, -9.80670575e-06,  4.64992718e-06,\n",
       "         3.41191408e-05, -1.70787989e-05,  2.13834574e-05, -1.59399588e-05,\n",
       "         1.92949847e-06, -4.38323805e-05, -7.57900079e-06, -3.40058705e-07,\n",
       "         1.59414631e-05, -4.54728361e-05, -2.49687946e-05,  3.79407829e-05,\n",
       "        -2.29751822e-05, -1.38957421e-05, -9.07674803e-06,  2.55422310e-06,\n",
       "         2.68055483e-05, -7.37779046e-05,  1.72989367e-05,  4.00235058e-06,\n",
       "         7.79871698e-07,  3.73360454e-05,  1.70748208e-05,  6.12921576e-06,\n",
       "        -1.31490779e-05,  1.04830571e-04,  2.47733697e-05, -7.97682515e-06,\n",
       "         1.12036450e-05, -6.34470634e-05,  6.71798116e-05,  2.67494088e-05,\n",
       "        -4.35792645e-05,  3.21193766e-05, -1.94975037e-05,  1.76920730e-06,\n",
       "        -8.95208359e-06,  1.50965134e-05, -1.33005451e-05,  4.37368953e-06,\n",
       "         2.38185494e-05,  7.95917949e-05,  2.74639278e-05,  6.42078739e-05,\n",
       "         7.17504154e-05, -2.60208944e-05, -4.80363160e-05, -5.04618729e-05,\n",
       "         1.11597819e-05,  1.01941070e-04, -3.26735280e-05,  1.41599507e-04,\n",
       "        -5.29088493e-06, -6.53614261e-05, -2.02389674e-05, -1.45051190e-05,\n",
       "         8.06961907e-05,  2.21618993e-05,  2.52636892e-06,  8.02025752e-05,\n",
       "         2.16084172e-05,  3.21120315e-05,  5.84927875e-06,  4.55067129e-05,\n",
       "        -3.96586001e-05,  1.60855489e-05,  6.52410235e-05,  5.75824379e-05,\n",
       "         4.38452007e-05,  2.85061524e-05,  3.79753801e-05,  1.13455244e-05,\n",
       "         3.01510408e-05,  4.06334693e-05, -1.26347964e-04, -2.39573274e-05,\n",
       "         2.53092730e-05, -5.18338857e-05, -6.37709891e-05,  7.65780715e-05,\n",
       "        -1.25090564e-05,  8.14221203e-05, -1.24898961e-05,  3.60769009e-05,\n",
       "        -4.11715246e-06,  3.04820351e-05, -1.43245688e-05, -4.30154523e-06,\n",
       "         5.71307828e-06, -2.89595628e-05, -2.43832928e-05,  1.17929798e-04,\n",
       "        -6.08248683e-07, -1.12459038e-05,  4.13469752e-05,  2.52555365e-05,\n",
       "         1.70916974e-06,  2.16947446e-05,  3.22071173e-06,  1.70495568e-05,\n",
       "        -2.52753016e-05,  4.43016143e-06, -1.33368267e-05,  2.20624861e-06,\n",
       "        -5.60179833e-05,  4.50732296e-05,  4.60442388e-05, -2.20180409e-05,\n",
       "        -4.09037748e-05,  1.01963069e-05,  2.75044695e-05, -3.11256226e-05,\n",
       "        -6.43348631e-06,  3.26310255e-05,  3.49882357e-05, -5.20063695e-05,\n",
       "        -1.91484469e-06,  6.33736636e-05,  4.54645633e-05,  3.33781463e-05,\n",
       "         3.93177215e-05, -2.55373889e-05,  1.72276032e-05,  3.04722671e-05,\n",
       "        -1.49775988e-05,  3.99141391e-05, -3.32201162e-05,  1.58910589e-05,\n",
       "         3.44203327e-05, -3.59550104e-05,  3.09306852e-05, -5.33117272e-05,\n",
       "         5.75141075e-06, -4.56216367e-05, -2.33805222e-05,  2.49429504e-05,\n",
       "         1.66744048e-05, -5.82080611e-05, -2.61530349e-05,  5.46894298e-05,\n",
       "        -3.03255431e-02, -9.27280821e-03,  3.71255726e-03,  1.77572239e-02,\n",
       "         5.14910445e-02, -3.57702002e-02, -4.70069237e-04, -6.66274503e-03,\n",
       "        -8.27321783e-02,  3.34014781e-02,  2.59053279e-02, -1.37947649e-02,\n",
       "        -2.39729136e-02, -3.42800766e-02,  5.72739821e-03,  4.46246332e-03,\n",
       "         2.79650558e-03, -2.05830857e-02, -8.11454374e-05, -2.69050878e-02,\n",
       "        -5.50746471e-02,  5.21034822e-02,  9.85850580e-04, -9.43125784e-03,\n",
       "         1.31082674e-02,  1.35403010e-03,  2.55269222e-02,  4.29406054e-02,\n",
       "         6.23494983e-02,  5.11969700e-02, -2.38488335e-02,  3.65090445e-02,\n",
       "        -2.85569802e-02, -2.14523263e-02,  5.65223210e-03, -1.51455291e-02,\n",
       "         3.65008623e-03,  2.58169994e-02, -4.89205234e-02, -6.49816617e-02,\n",
       "        -9.87647194e-03,  3.38237509e-02, -1.67735200e-02, -2.65226886e-03,\n",
       "         1.76485032e-02,  5.01379371e-03, -1.44388638e-02,  3.87708023e-02,\n",
       "         4.87057269e-02, -2.98601221e-02,  1.07009793e-02, -1.52412616e-02,\n",
       "         4.82623372e-03, -1.52285649e-02, -4.17344347e-02,  2.16019526e-02,\n",
       "         2.36267187e-02,  1.77263133e-02,  2.29721293e-02,  6.43844204e-03,\n",
       "        -1.34658730e-02, -2.40064207e-02,  5.10220528e-02, -3.77224609e-02,\n",
       "         8.65941495e-03, -4.34512421e-02, -4.48486134e-02, -4.85840291e-02,\n",
       "        -1.17153926e-02, -5.46478033e-02, -2.33373838e-03, -3.42414454e-02,\n",
       "         8.95448402e-03,  1.05112605e-02,  1.85862929e-03, -3.18704620e-02,\n",
       "        -2.39404570e-03,  1.63655840e-02, -1.59465112e-02, -4.01078798e-02,\n",
       "         4.01170505e-03,  4.74918075e-03,  3.44600230e-02,  2.41945721e-02,\n",
       "         9.16864909e-03, -4.48442250e-03, -6.13810960e-04,  2.01666225e-02,\n",
       "        -7.70002045e-03,  1.24781672e-02,  4.42587622e-02,  2.84377113e-02,\n",
       "        -1.06466180e-02, -8.45859386e-03, -2.43186355e-02,  3.09462585e-02,\n",
       "        -1.54787097e-02,  2.26804093e-02, -3.26688215e-02,  5.60978148e-03,\n",
       "        -2.98607107e-02,  3.63994054e-02,  7.74438353e-03,  6.11351579e-02,\n",
       "        -5.06837443e-02,  3.97911519e-02, -2.87216268e-02, -1.49703790e-02,\n",
       "         2.95817293e-03, -6.10448867e-02,  7.61223165e-03,  3.44506465e-04,\n",
       "        -9.53020900e-03,  6.49862224e-03, -2.68518645e-03,  1.53880185e-02,\n",
       "         5.75920474e-03, -2.49681436e-02,  9.58592631e-04, -4.78258543e-03,\n",
       "        -4.18232791e-02, -1.26932543e-02,  1.51064862e-02, -7.26579223e-03,\n",
       "         2.92200968e-02,  2.78978460e-02,  2.39193235e-02,  1.85779277e-02,\n",
       "        -2.43629984e-05, -6.02703767e-06, -1.35558384e-05, -2.50664243e-06,\n",
       "         2.22862618e-05, -5.16405125e-05,  2.12392806e-05, -4.36396840e-06,\n",
       "        -3.45555454e-05,  2.50757003e-05,  1.14835893e-05, -1.31005954e-05,\n",
       "         1.34986403e-05,  6.78115321e-05,  1.20450604e-05, -1.35948194e-05,\n",
       "         2.18889490e-05, -4.41627017e-05,  6.93055554e-05,  1.40105531e-05,\n",
       "        -3.23589520e-05,  2.34813378e-05, -3.09780398e-06, -2.93753892e-05,\n",
       "        -3.65427040e-05,  1.45666663e-05, -3.06527982e-05,  1.65969068e-05,\n",
       "         2.02926331e-05,  4.88623809e-05,  3.84354789e-05,  2.01445346e-05,\n",
       "         4.97662768e-05, -4.05573737e-05, -1.71466381e-05, -1.95012781e-05,\n",
       "         9.70386100e-06,  7.79475013e-05, -2.99746625e-05,  1.33306428e-04,\n",
       "        -1.09388848e-05, -2.04463431e-05, -1.70588646e-05, -5.08181165e-06,\n",
       "         6.53427342e-05,  1.71009015e-06, -8.20731384e-06,  5.43849274e-05,\n",
       "         2.18942623e-05,  2.71695899e-05,  1.96632755e-06,  3.04822606e-05,\n",
       "        -3.71831084e-05, -2.24520454e-06,  4.20209435e-05,  4.58329923e-05,\n",
       "         2.42368078e-05,  4.79995069e-05,  3.75350937e-05, -3.10702671e-06,\n",
       "         3.56249657e-05,  1.67793278e-05, -7.97668836e-05, -8.14832219e-06,\n",
       "         6.29496753e-06, -4.57860515e-05, -4.03206977e-05,  5.65566443e-05,\n",
       "        -2.97182232e-05,  5.46251140e-05,  6.23133974e-06,  1.54133850e-05,\n",
       "        -7.13064583e-06,  3.11227705e-05, -1.97783884e-05,  4.71117801e-06,\n",
       "        -2.79761252e-06, -4.02858823e-05, -2.02769297e-05,  7.14677590e-05,\n",
       "        -2.55518898e-05, -2.46356558e-05,  2.51447309e-05,  2.21545306e-05,\n",
       "        -1.17082964e-05, -8.47913543e-06,  7.37673508e-06,  2.30570186e-05,\n",
       "        -3.48263457e-05,  1.39586955e-05, -5.30526449e-07,  2.10971521e-05,\n",
       "        -5.08858429e-05,  5.16670625e-05,  2.43848172e-05, -2.65299532e-05,\n",
       "        -3.92839029e-05,  5.39301709e-06,  1.94434633e-05, -2.39071887e-05,\n",
       "        -6.51875325e-06, -1.06330426e-05,  2.88253941e-05, -3.77258766e-05,\n",
       "        -1.99564765e-05,  4.63582837e-05,  4.05097016e-05,  3.62673854e-05,\n",
       "         5.30836041e-05, -1.81459782e-05,  5.17266926e-07,  3.17030535e-05,\n",
       "         4.33244986e-06,  2.40426998e-05, -2.13108942e-05, -4.55484223e-06,\n",
       "         3.33145108e-05, -1.63590557e-05,  2.70043383e-05, -1.78784194e-05,\n",
       "         8.19086381e-07, -4.33043133e-05, -9.21024912e-06,  1.48488834e-06,\n",
       "         1.14691629e-05, -4.69654879e-05, -2.16555400e-05,  4.18804557e-05],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(128, 10000), dtype=float32, numpy=\n",
       " array([[-2.5997562e-03, -3.1275689e-03, -1.5074344e-03, ...,\n",
       "          3.8862872e-06,  3.8878334e-06,  3.8874286e-06],\n",
       "        [ 1.2456398e-03,  1.8508717e-03,  2.0183986e-03, ...,\n",
       "         -3.0386054e-06, -3.0402402e-06, -3.0391072e-06],\n",
       "        [ 1.6745620e-03,  2.5880071e-03,  1.6192558e-03, ...,\n",
       "         -4.3551618e-06, -4.3559039e-06, -4.3573559e-06],\n",
       "        ...,\n",
       "        [ 3.2339629e-04,  4.5403844e-04,  4.4332299e-04, ...,\n",
       "         -4.2573348e-08, -3.8772455e-08, -4.3857213e-08],\n",
       "        [ 2.4436420e-05,  2.8553896e-04, -7.4503332e-04, ...,\n",
       "          2.9450746e-08,  2.9750765e-08,  2.8350428e-08],\n",
       "        [-3.2629148e-05,  6.9734547e-04, -9.6071913e-04, ...,\n",
       "          5.2688722e-07,  5.2590434e-07,  5.2768053e-07]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10000,), dtype=float32, numpy=\n",
       " array([-0.7979985 , -1.0313317 , -1.0313317 , ...,  0.00200004,\n",
       "         0.00200058,  0.00200059], dtype=float32)>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the gradient clipping threshold\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M3rshV_SEQk"
   },
   "source": [
    "<h4> 4.Apply the optimizer to the variables/gradients tuple. </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94kcBU_NSEQk"
   },
   "outputs": [],
   "source": [
    "# Create the training TensorFlow Operation through our optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjURp3p-SEQk"
   },
   "source": [
    "<a id=\"ltsm\"></a>\n",
    "\n",
    "<h2>LSTM</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqVOh66fSEQl"
   },
   "source": [
    "let's then create a Class that represents our model. This class needs a few things:\n",
    "\n",
    "<ul>\n",
    "    <li>We have to create the model in accordance with our defined hyperparameters</li>\n",
    "    <li>We have to create the LSTM cell structure and connect them with our RNN structure</li>\n",
    "    <li>We have to create the word embeddings and point them to the input data</li>\n",
    "    <li>We have to create the input structure for our RNN</li>\n",
    "    <li>We need to create a logistic structure to return the probability of our words</li>\n",
    "    <li>We need to create the loss and cost functions for our optimizer to work, and then create the optimizer</li>\n",
    "    <li>And finally, we need to create a training operation that can be run to actually train our model</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5J5Anb_vSEQl"
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = 1.0\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Initializing the model using keras Sequential API  #\n",
    "        ###############################################################################\n",
    "        \n",
    "        self._model = tf.keras.models.Sequential()\n",
    "        \n",
    "        ####################################################################\n",
    "        # Creating the word embeddings layer and adding it to the sequence #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            self._embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embeding_vector_size,batch_input_shape=(self.batch_size, self.num_steps),trainable=True,name=\"embedding_vocab\")  #[10000x200]\n",
    "            self._model.add(self._embedding_layer)\n",
    "            \n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM Cells. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument  of LSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A). \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
    "        lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
    "        lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)\n",
    "        \n",
    "\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the StackedRNNCells function junctions the LSTM units to the RNN units.\n",
    "        # RNN cell composed sequentially of stacked simple cells.\n",
    "        stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
    "\n",
    "        ########################################################################################################\n",
    "        # Instantiating our RNN model and setting stateful to True to feed forward the state to the next layer #\n",
    "        ########################################################################################################\n",
    "        \n",
    "        self._RNNlayer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)\n",
    "        \n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)\n",
    "        self._RNNlayer.inital_state = self._initial_state\n",
    "    \n",
    "        ############################################\n",
    "        # Adding RNN layer to keras sequential API #\n",
    "        ############################################        \n",
    "        self._model.add(self._RNNlayer)\n",
    "        \n",
    "        #self._model.add(tf.keras.layers.LSTM(hidden_size_l1,return_sequences=True,stateful=True))\n",
    "        #self._model.add(tf.keras.layers.LSTM(hidden_size_l2,return_sequences=True))\n",
    "        \n",
    "        \n",
    "        ####################################################################################################\n",
    "        # Instantiating a Dense layer that connects the output to the vocab_size  and adding layer to model#\n",
    "        ####################################################################################################\n",
    "        self._dense = tf.keras.layers.Dense(self.vocab_size)\n",
    "        self._model.add(self._dense)\n",
    " \n",
    "        \n",
    "        ####################################################################################################\n",
    "        # Adding softmax activation layer and deriving probability to each class and adding layer to model #\n",
    "        ####################################################################################################\n",
    "        self._activation = tf.keras.layers.Activation('softmax')\n",
    "        self._model.add(self._activation)\n",
    "\n",
    "        ##########################################################\n",
    "        # Instantiating the stochastic gradient decent optimizer #\n",
    "        ########################################################## \n",
    "        self._optimizer = tf.keras.optimizers.SGD(lr=self._lr, clipnorm=max_grad_norm)\n",
    "        \n",
    "        \n",
    "        ##############################################################################\n",
    "        # Compiling and summarizing the model stacked using the keras sequential API #\n",
    "        ##############################################################################\n",
    "        self._model.compile(loss=self.crossentropy, optimizer=self._optimizer)\n",
    "        self._model.summary()\n",
    "\n",
    "\n",
    "    def crossentropy(self,y_true, y_pred):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    def train_batch(self,_input_data,_targets):\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = self._model.trainable_variables\n",
    "        # Define the gradient clipping threshold\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass.\n",
    "            output_words_prob = self._model(_input_data)\n",
    "            # Loss value for this batch.\n",
    "            loss  = self.crossentropy(_targets, output_words_prob)\n",
    "            # average across batch and reduce sum\n",
    "            cost = tf.reduce_sum(loss/ self.batch_size)\n",
    "        # Get gradients of loss wrt the trainable variables.\n",
    "        grad_t_list = tape.gradient(cost, tvars)\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        train_op = self._optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return cost\n",
    "        \n",
    "    def test_batch(self,_input_data,_targets):\n",
    "        #################################################\n",
    "        # Creating the Testing Operation for our Model #\n",
    "        #################################################\n",
    "        output_words_prob = self._model(_input_data)\n",
    "        loss  = self.crossentropy(_targets, output_words_prob)\n",
    "        # average across batch and reduce sum\n",
    "        cost = tf.reduce_sum(loss/ self.batch_size)\n",
    "\n",
    "        return cost\n",
    "    @classmethod\n",
    "    def instance(cls) : \n",
    "        return PTBModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0US9jQEGSEQl"
   },
   "source": [
    "With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to actually create the methods to run through time -- that is, the <code>run_epoch</code> method to be run at each epoch and a <code>main</code> script which ties all of this together.\n",
    "\n",
    "What our <code>run_epoch</code> method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSce05vESEQl"
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################################\n",
    "# run_one_epoch takes as parameters  the model instance, the data to be fed, training or testing mode and verbose info #\n",
    "########################################################################################################################\n",
    "def run_one_epoch(m, data,is_training=True,verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.\n",
    "    iters = 0\n",
    "    \n",
    "    m._model.reset_states()\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        #y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "        if is_training : \n",
    "            loss=  m.train_batch(x, y)\n",
    "        else :\n",
    "            loss = m.test_batch(x, y)\n",
    "                                   \n",
    "\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += loss\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "        \n",
    "\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgN_6A5uSEQl"
   },
   "source": [
    "Now, we create the <code>main</code> method to tie everything together. The code here reads the data from the directory, using the <code>reader</code> helper module, and then trains and evaluates the model on both a testing and a validating subset of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FE42owcRSEQm"
   },
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PbMXh0zSEQm",
    "outputId": "b977c1ad-5a68-4d88-dcb7-0c7eab98a55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_vocab (Embedding)  (30, 20, 200)             2000000   \n",
      "_________________________________________________________________\n",
      "rnn_1 (RNN)                  (30, 20, 128)             671088    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (30, 20, 10000)           1290000   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (30, 20, 10000)           0         \n",
      "=================================================================\n",
      "Total params: 3,961,088\n",
      "Trainable params: 3,955,088\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n",
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 4722.261 speed: 963 wps\n",
      "Itr 164 of 1549, perplexity: 1092.543 speed: 988 wps\n",
      "Itr 318 of 1549, perplexity: 850.528 speed: 993 wps\n",
      "Itr 472 of 1549, perplexity: 702.228 speed: 992 wps\n",
      "Itr 626 of 1549, perplexity: 598.777 speed: 993 wps\n",
      "Itr 780 of 1549, perplexity: 531.432 speed: 989 wps\n",
      "Itr 934 of 1549, perplexity: 479.042 speed: 989 wps\n",
      "Itr 1088 of 1549, perplexity: 439.844 speed: 988 wps\n",
      "Itr 1242 of 1549, perplexity: 409.676 speed: 989 wps\n",
      "Itr 1396 of 1549, perplexity: 381.650 speed: 990 wps\n",
      "Epoch 1 : Train Perplexity: 359.493\n",
      "Epoch 1 : Valid Perplexity: 212.917\n",
      "Epoch 2 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 239.601 speed: 983 wps\n",
      "Itr 164 of 1549, perplexity: 213.227 speed: 978 wps\n",
      "Itr 318 of 1549, perplexity: 203.907 speed: 984 wps\n",
      "Itr 472 of 1549, perplexity: 195.519 speed: 985 wps\n",
      "Itr 626 of 1549, perplexity: 186.519 speed: 987 wps\n",
      "Itr 780 of 1549, perplexity: 182.584 speed: 988 wps\n",
      "Itr 934 of 1549, perplexity: 178.350 speed: 989 wps\n",
      "Itr 1088 of 1549, perplexity: 174.871 speed: 989 wps\n",
      "Itr 1242 of 1549, perplexity: 172.403 speed: 987 wps\n",
      "Itr 1396 of 1549, perplexity: 168.204 speed: 987 wps\n",
      "Epoch 2 : Train Perplexity: 165.335\n",
      "Epoch 2 : Valid Perplexity: 160.429\n",
      "Epoch 3 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 163.480 speed: 984 wps\n",
      "Itr 164 of 1549, perplexity: 147.912 speed: 989 wps\n",
      "Itr 318 of 1549, perplexity: 144.148 speed: 989 wps\n",
      "Itr 472 of 1549, perplexity: 139.352 speed: 991 wps\n",
      "Itr 626 of 1549, perplexity: 134.169 speed: 991 wps\n",
      "Itr 780 of 1549, perplexity: 132.863 speed: 991 wps\n",
      "Itr 934 of 1549, perplexity: 131.056 speed: 991 wps\n",
      "Itr 1088 of 1549, perplexity: 129.541 speed: 992 wps\n",
      "Itr 1242 of 1549, perplexity: 128.715 speed: 992 wps\n",
      "Itr 1396 of 1549, perplexity: 126.425 speed: 992 wps\n",
      "Epoch 3 : Train Perplexity: 125.107\n",
      "Epoch 3 : Valid Perplexity: 145.600\n",
      "Epoch 4 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 130.236 speed: 980 wps\n",
      "Itr 164 of 1549, perplexity: 120.371 speed: 992 wps\n",
      "Itr 318 of 1549, perplexity: 118.139 speed: 994 wps\n",
      "Itr 472 of 1549, perplexity: 114.620 speed: 994 wps\n",
      "Itr 626 of 1549, perplexity: 110.708 speed: 993 wps\n",
      "Itr 780 of 1549, perplexity: 110.114 speed: 993 wps\n",
      "Itr 934 of 1549, perplexity: 108.974 speed: 992 wps\n",
      "Itr 1088 of 1549, perplexity: 108.128 speed: 991 wps\n",
      "Itr 1242 of 1549, perplexity: 107.757 speed: 991 wps\n",
      "Itr 1396 of 1549, perplexity: 106.145 speed: 991 wps\n",
      "Epoch 4 : Train Perplexity: 105.388\n",
      "Epoch 4 : Valid Perplexity: 137.589\n",
      "Epoch 5 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 112.837 speed: 982 wps\n",
      "Itr 164 of 1549, perplexity: 104.826 speed: 987 wps\n",
      "Itr 318 of 1549, perplexity: 103.298 speed: 989 wps\n",
      "Itr 472 of 1549, perplexity: 100.309 speed: 990 wps\n",
      "Itr 626 of 1549, perplexity: 96.981 speed: 991 wps\n",
      "Itr 780 of 1549, perplexity: 96.709 speed: 991 wps\n",
      "Itr 934 of 1549, perplexity: 95.901 speed: 991 wps\n",
      "Itr 1088 of 1549, perplexity: 95.300 speed: 991 wps\n",
      "Itr 1242 of 1549, perplexity: 95.154 speed: 992 wps\n",
      "Itr 1396 of 1549, perplexity: 93.897 speed: 992 wps\n",
      "Epoch 5 : Train Perplexity: 93.352\n",
      "Epoch 5 : Valid Perplexity: 134.324\n",
      "Epoch 6 : Learning rate: 0.500\n",
      "Itr 10 of 1549, perplexity: 100.041 speed: 963 wps\n",
      "Itr 164 of 1549, perplexity: 91.507 speed: 994 wps\n",
      "Itr 318 of 1549, perplexity: 88.823 speed: 995 wps\n",
      "Itr 472 of 1549, perplexity: 85.454 speed: 995 wps\n",
      "Itr 626 of 1549, perplexity: 81.726 speed: 994 wps\n",
      "Itr 780 of 1549, perplexity: 80.902 speed: 995 wps\n",
      "Itr 934 of 1549, perplexity: 79.661 speed: 994 wps\n"
     ]
    }
   ],
   "source": [
    "# Instantiates the PTBModel class\n",
    "m=PTBModel.instance()   \n",
    "K = tf.keras.backend \n",
    "for i in range(max_epoch):\n",
    "    # Define the decay for this epoch\n",
    "    lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "    dcr = learning_rate * lr_decay\n",
    "    m._lr = dcr\n",
    "    K.set_value(m._model.optimizer.learning_rate,m._lr)\n",
    "    print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, m._model.optimizer.learning_rate))\n",
    "    # Run the loop for this epoch in the training mode\n",
    "    train_perplexity = run_one_epoch(m, train_data,is_training=True,verbose=True)\n",
    "    print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "    # Run the loop for this epoch in the validation mode\n",
    "    valid_perplexity = run_one_epoch(m, valid_data,is_training=False,verbose=False)\n",
    "    print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "# Run the loop in the testing mode to see how effective was our training\n",
    "test_perplexity = run_one_epoch(m, test_data,is_training=False,verbose=False)\n",
    "print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXEfoJzfSEQm"
   },
   "source": [
    "As you can see, the model's perplexity rating drops very quickly after a few iterations. As was elaborated before, <b>lower Perplexity means that the model is more certain about its prediction</b>. As such, we can be sure that this model is performing well!\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "Language Modelling LSTM .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
